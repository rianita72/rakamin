# -*- coding: utf-8 -*-
"""Final_Project_Holiday_Package_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cJ3nG0jOShEn1RMk7Q0k-8uA6T9pZWA0

[https://github.com/rianita72/rakamin_project]()

# Dataset
"Trips & Travel.Com" company wants to enable and establish a viable business model to expand the customer base. One of the ways to expand the customer base is to introduce a new offering of packages. Currently, there are 5 types of packages the company is offering - Basic, Standard, Deluxe, Super Deluxe, King. Looking at the data of the last year, we observed that 18% of the customers purchased the packages. However, the marketing cost was quite high because customers were contacted at random without looking at the available information. The company is now planning to launch a new product i.e. Wellness Tourism Package. Wellness Tourism is defined as Travel that allows the traveler to maintain, enhance or kick-start a healthy lifestyle, and support or increase one's sense of well-being. However, this time company wants to harness the available data of existing and potential customers to make the marketing expenditure more efficient.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from scipy import stats
import math



import gc

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
df = pd.read_csv('/content/gdrive/My Drive/Rakamin/Travel.csv')

"""Flow:
- Data exploration
- EDA
- Business insight
- Handling Missing Value
- Handling Duplicated Data
- Handling Invalid Values
- Feature Extraction
- Feature Selection
- Feature Transformation (Numeric)
- Feature Encoding (Categoric)
- Handling Imbalance
- Modelling
- Evaluation
-Â Visualization

# Exploratory Data Analysis
"""

df.sample(10, random_state=100)

df.head()

"""## Descriptive Statistics"""

df.info()

"""1. CustomerID: ID unik untuk setiap pelanggan. 
2. ProdTaken: Indikator biner (0 atau 1) yang menunjukkan apakah pelanggan telah membeli paket liburan (1) atau tidak (0).
3. Age: Usia pelanggan.
4. TypeofContact: Jenis kontak yang digunakan untuk menghubungi pelanggan (Value; 'Company Invited' atau 'Self Enquiry').
5. CityTier: Tingkat kota tempat tinggal pelanggan (mengacu pada tingkat perkembangan atau status kota)(Value; 1-3).
6. DurationOfPitch: Durasi presentasi dalam menit saat menawarkan paket liburan kepada pelanggan.
7. Occupation: Pekerjaan pelanggan ('Salaried', 'Small Business', atau 'Large Business').
8. Gender: Jenis kelamin pelanggan(Male dan Female).
9. NumberOfPersonVisiting: Jumlah orang yang akan mengunjungi (berkaitan dengan jumlah anggota keluarga atau rekan bisnis yang akan ikut dalam perjalanan liburan).
10. NumberOfFollowups: Jumlah tindak lanjut yang telah dilakukan dengan pelanggan.
11. ProductPitched: Produk yang ditawarkan kepada pelanggan.(Basic, Deluxe, Standard, Super Deluxe, King)
12. PreferredPropertyStar: Tingkat bintang properti yang diinginkan oleh pelanggan.(Value;Bintang 3-5)
13. MaritalStatus: Status perkawinan pelanggan.(Single,Married,Divorce,Unmaried)
14. NumberOfTrips: Jumlah perjalanan yang telah dilakukan oleh pelanggan sebelumnya.
15. Passport: Indikator biner (0 atau 1) yang menunjukkan apakah pelanggan memiliki paspor (1) atau tidak (0).
16. PitchSatisfactionScore: Skor kepuasan pelanggan terhadap presentasi penawaran.(Value;1-5)
17. OwnCar: Indikator biner (0 atau 1) yang menunjukkan apakah pelanggan memiliki mobil (1) atau tidak (0).
18. NumberOfChildrenVisiting: Jumlah anak yang ikut dalam perjalanan liburan.
19. Designation: Jabatan atau posisi pekerjaan pelanggan.(Executive, Manager, Senior Manager, AVP, VP)
20. MonthlyIncome: Pendapatan bulanan pelanggan.

1. Terdapat kesalahan penulisan pada kolom Gender dimana 'Fe male' seharusnya 'Female'
2. Terdapat penggunaan istilah yang berbeda pada 'Unmarried' dan 'Single' dimana kedua status itu sama
"""

num_cols = df.select_dtypes(exclude='object').columns.tolist()
cat_cols = df.select_dtypes(include='object').columns.tolist()

df[num_cols].describe().T

"""1. Jumlah baris dalam dataset adalah 4.888.

2. Terdapat beberapa kolom yang memiliki missing value, yaitu kolom Age, DurationOfPitch, NumberOfTrips, PreferredPropertyStar, dan MonthlyIncome. Kolom-kolom ini perlu ditangani dengan metode pengisian nilai yang sesuai sebelum dilakukan analisis lebih lanjut.

3. Rata-rata usia (Age) pelanggan dalam dataset adalah 37.62 tahun dengan standar deviasi sebesar 9.32. Distribusi usia cenderung mendekati distribusi normal.

4. Durasi presentasi (DurationOfPitch), jumlah perjalanan (NumberOfTrips), dan pendapatan bulanan (MonthlyIncome) memiliki skewness positif, yang menunjukkan adanya pencilan atau outlier dalam data.

5. Sebagian besar pelanggan (75%) berada di CityTier 3.

6. Mayoritas pelanggan tidak memiliki paspor (Passport) dan tidak memiliki mobil pribadi (OwnCar).

7. Nilai kepuasan terhadap presentasi (PitchSatisfactionScore) rata-rata adalah 3.08 dari skala 1 hingga 5.

8. Mayoritas pelanggan (75%) memiliki jumlah anak yang mengunjungi (NumberOfChildrenVisiting) sebanyak 2.

9. Pendapatan bulanan (MonthlyIncome) rata-rata pelanggan adalah sebesar 23,619.85

10. Terdapat kolom-kolom diskrit atau ordinal lainnya yang tidak mencerminkan distribusi data tertentu.
"""

df[cat_cols].describe().T

"""1. Pada kolom TypeofContact terdapat missing value
2. Pada kolom TypeofContact dan Gender terdapat data yang tidak seimbang
"""

'''#Mengecek Skewness
from scipy.stats import skew

skewness = df[num_cols].apply(skew)
print(skewness)'''

for col in cat_cols:
    print(f'''Value count kolom {col}:''')
    print(df[col].value_counts())
    print()

"""## Univariate Analysis"""

# define numerical & categorical columns

plt.figure(figsize=(12,8))
for i in range(0, len(num_cols)):
    plt.subplot(3, 6, i+1)
    sns.boxplot(y=df[num_cols[i]], x=df['ProdTaken'],showfliers=False , orient='v')
    plt.tight_layout()

num_cols_count = len(num_cols)
num_rows = math.ceil(num_cols_count / 3)
num_figures = math.ceil(num_cols_count / 9)

for f in range(num_figures):
    start_idx = f * 9
    end_idx = min(start_idx + 9, num_cols_count)
    plt.figure(figsize=(18, 15))
    for i, col in enumerate(num_cols[start_idx:end_idx]):
        plt.subplot(num_rows, 3, i+1)
        sns.histplot(df[col].dropna(), kde=True, color='skyblue')
        plt.xlabel(col)
        plt.ylabel('Count')
    plt.tight_layout()
    plt.show()

"""Kesimpulan :
1. Distribusi kolom Age cenderung mendekati distribusi normal, menunjukkan penyebaran nilai yang seimbang.
2. kolom DurationOfPitch, NumberOfTrips, dan MonthlyIncome memiliki skewness positif, menunjukkan adanya pencilan atau outlier dalam data. Pencilan ini dapat memiliki pengaruh yang signifikan terhadap hasil analisis.
3. Untuk kolom lainnya, dapat diabaikan karena umumnya mengandung data diskrit atau ordinal, yang tidak memerlukan pertimbangan yang sama dengan variabel kontinu.
"""

# Univariate analysis for categorical columns
num_cat_cols = len(cat_cols)
num_rows = (num_cat_cols - 1) // 3 + 1
plt.figure(figsize=(15, 5*num_rows))
for i, col in enumerate(cat_cols):
    plt.subplot(num_rows, 3, i+1)
    sns.countplot(x=col, data=df, hue=df['ProdTaken'])
    plt.xlabel(col)
    plt.ylabel('Count')
plt.tight_layout()
plt.show()

"""1. Pada kolom TypeofContact distribusi Self Enquiry lebih banyak daripada Company Visited
2. Pada kolom Occupation distribusi Salaried dan Small Business paling banyak dibanding yang lainnya, dan pada Free Lancer nantinya akan dihapus karena tidak ada nilainya
3. Pada kolom Gender jenis kelamin laki-laki lebih banyak dibandingkan perempuan. Dan ada kesalahan penulisan Fe Male yang dimana seharusnya Female.
4. Pada kolom ProductPitched basic dan Deluxe memiliki penjualan tertinggi
5. Pada kolom MaritalStatus Customer yang sudah menikah memiliki jumlah paling banyak
6. Pada kolom Designation customer dengan jabatan Executive dan Manager memiliki jumlah paling banyak

## Multivariate Analysis
"""

corr_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix Heatmap', fontsize=16, fontweight='bold')
plt.xticks(rotation=80)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""1. Kolom MonthlyIncome, Passport dan Age memiliki korelasi kuat terhadap kolom target (ProdTaken)
2. Kolom NumberOfChildrenVisiting dan NumberOfPersonVisiting memiliki korelasi kuat

## Business Insight and Visualization
"""

# Calculate the purchase percentage by Passport
purchase_percentage = df.groupby('ProdTaken').size() / len(df) * 100

# Plot the purchase percentage
plt.figure(figsize=(8, 6))
ax = sns.barplot(x=purchase_percentage.index, y=purchase_percentage.values, color='steelblue')
plt.xlabel('ProdTaken')
plt.ylabel('Purchase Percentage')
plt.title('Purchase Percentage')

# Display percentages on the bars
for i, value in enumerate(purchase_percentage):
    ax.text(i, value, f'{value:.2f}%', ha='center', va='bottom', color='black')

plt.show()

# Select the relevant columns for analysis
selected_columns = ['Gender', 'MaritalStatus', 'Occupation', 'ProdTaken']  # Modify with the desired columns
data = df[selected_columns]

# Filter only rows where product was taken (ProdTaken = 0)
data = data[data['ProdTaken'] == 0]

# Create cross-tabulation table
cross_tab = pd.crosstab(index=[data['Gender'], data['MaritalStatus']], columns=data['Occupation'])

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(cross_tab, cmap='YlGnBu', annot=True, fmt='d', cbar=True)
plt.xlabel('Occupation', fontsize=12)
plt.ylabel('Gender, Marital Status', fontsize=12)
plt.title('Gender, Marital Status, and Occupation Who Did Not Purchase', fontsize=14, fontweight='bold')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Select the relevant columns for analysis
selected_columns = ['Gender', 'MaritalStatus', 'Occupation', 'ProdTaken']  # Modify with the desired columns
data = df[selected_columns]

# Filter only rows where product was taken (ProdTaken = 1)
data = data[data['ProdTaken'] == 1]

# Create cross-tabulation table
cross_tab = pd.crosstab(index=[data['Gender'], data['MaritalStatus']], columns=data['Occupation'])

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(cross_tab, cmap='YlGnBu', annot=True, fmt='d', cbar=True)
plt.xlabel('Occupation', fontsize=12)
plt.ylabel('Gender, Marital Status', fontsize=12)
plt.title('Purchase of Product by Gender, Marital Status, and Occupation', fontsize=14, fontweight='bold')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""1. Berdasarkan visualisasi diatas, kebanyakan produk dibeli oleh laki-laki, sudah menikah dan wiraswasta

2. Berdasarkan visualisasi diatas, mayoritas yang tidak membeli produk adalah laki-laki yang sudah menikah dan bekerja sebagai karyawan, sedangkan mayoritas yang membeli produk adalah laki-laki yang sudah menikah dan seorang wiraswasta.
"""

# Segment age into categories
df['AgeCategory'] = pd.cut(df['Age'], bins=[0, 18, 25, 35, 50, 100], labels=['<18', '18-25', '26-35', '36-50', '50+'])

# Count the number of purchases by age category
purchase_counts = df.groupby('AgeCategory')['ProdTaken'].sum().reset_index()

# Calculate the total purchases
total_purchases = purchase_counts['ProdTaken'].sum()

# Calculate the percentage of purchases by age category
purchase_counts['Percentage'] = purchase_counts['ProdTaken'] / total_purchases * 100

# Plot the purchase counts by age category
plt.figure(figsize=(8, 6))
ax = sns.barplot(x='AgeCategory', y='ProdTaken', data=purchase_counts, color='steelblue')
plt.xlabel('Age Category')
plt.ylabel('Number of Purchases')
plt.title('Number of Purchases by Age Category')

# Annotate percentages on the bars
for i, row in purchase_counts.iterrows():
    percentage = row['Percentage']
    ax.annotate(f'{percentage:.2f}%', xy=(i, row['ProdTaken']), ha='center', va='bottom', color='black')

plt.show()
df.drop('AgeCategory', axis=1, inplace=True)

"""1.  Berdasarkan visualisasi diatas, pembeli produk rata-rata berumur 26-50

Karena mayoritas yang membeli produk adalah anak muda, maka kita bisa memaksimalkan promosi melalui media sosial seperti Instagram, Tiktok, Ataupun Facebook dimana mayoritas penggunanya adalah anak muda
"""

# Calculate the purchase percentage by CityTier
purchase_percentage = df.groupby('CityTier')['ProdTaken'].mean() * 100

# Plot the purchase percentage by CityTier
plt.figure(figsize=(8, 6))
ax = sns.barplot(x=purchase_percentage.index, y=purchase_percentage.values, color='steelblue')
plt.xlabel('CityTier')
plt.ylabel('Purchase Percentage')
plt.title('Purchase Percentage by CityTier')

# Display percentages on the bars
for i, value in enumerate(purchase_percentage):
    ax.text(i, value, f'{value:.2f}%', ha='center', va='bottom', color='black')

plt.show()

"""1. Berdasarkan visualisasi diatas, kebanyakan transaksi dilakukan oleh pelanggan yang berasal dari CityTier 2 dan 3

Karena kebanyakan pembeli tinggal dilingkungan kota yang sudah maju atau juga tinggal di kota satelit maka kita dapat mengkampanyekan banyak produk di area tersebut mengingat daya beli mereka yang cukup tinggi.
"""

# Calculate the purchase percentage by CityTier
purchase_percentage = df.groupby('Passport')['ProdTaken'].mean() * 100

# Plot the purchase percentage by CityTier
plt.figure(figsize=(8, 6))
ax = sns.barplot(x=purchase_percentage.index, y=purchase_percentage.values, color='steelblue')
plt.xlabel('Passport')
plt.ylabel('Purchase Percentage')
plt.title('Purchase Percentage by Passport')

# Display percentages on the bars
for i, value in enumerate(purchase_percentage):
    ax.text(i, value, f'{value:.2f}%', ha='center', va='bottom', color='black')

plt.show()

"""1. Berdasarkan visualisasi diatas, kebanyakan transaksi dilakukan oleh orang yang memiliki passport.

Kita bisa menawarkan jasa pembuatan pasport sebagai bundling product ketika melakukan promosi paket travel.
"""

# Plot the Average Duration of Picth by ProdTaken
plt.figure(figsize=(8, 6))
ax = sns.barplot(x='ProdTaken', y='DurationOfPitch', data=df,color='steelblue')
plt.xlabel('ProdTaken')
plt.ylabel('DurationOfPitch')
plt.title('Average Duration of Picth by ProdTaken')


plt.show()

"""Dapat dilihat bahwa rata-rata durasi pitching customer yang membeli paket hanya sedikit lebih tinggi daripada rata-rata durasi pitching customer yang tidak membeli."""

# Plot the Number of Follow Ups by ProdTaken
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))
sns.countplot(x='NumberOfFollowups', data=df,hue='ProdTaken', color='steelblue',ax=ax1)
plt.xlabel('NumberOfFollowups')
ax1.title.set_text('Number of Follow Ups by ProdTaken')

# Calculate the purchase percentage by CityTier
purchase_percentage = df.groupby('NumberOfFollowups')['ProdTaken'].mean() * 100

# Plot the purchase percentage by Number of Follow Ups
sns.barplot(x=purchase_percentage.index, y=purchase_percentage.values, color='steelblue',ax=ax2)
plt.xlabel('Number of Follow Ups')
plt.ylabel('Purchase Percentage')
ax2.title.set_text('Purchase Percentage by Number of Follow Ups')

# Display percentages on the bars
for i, value in enumerate(purchase_percentage):
    ax2.text(i, value, f'{value:.2f}%', ha='center', va='bottom', color='black')

plt.show()

"""Dari visualisasi di atas dapat dilihat bahwa total follow up kurang dari 6 kali memiliki total customer yang tidak membeli jauh lebih tinggi dibandingkan customer yang di-follow up sebanyak 6 kali dengan conversion rate tertinggi yaitu 39,71%.

# Data Preprocessing

## Handling missing value
"""

df_copy = df.copy()

# Cek jumlah missing value dalam setiap kolom
missing_values = df_copy.isnull().sum()
print(missing_values)

"""### Opsi 1: Mean and Median"""

df_mv1 = df.copy()

"""Berdasarkan pengamatan EDA maka:
*   Kolom **Age** akan diisi dengan nilai mean
*   Kolom **DurationOfPitch** karena skewed atau terdapat outlier akan diisi oleh nilai median
*   Kolom **NumberOfFollowups** akan diisi oleh nilai mean
*   Kolom **PreferredPropertyStar** akan diisi oleh nilai mean
*   Kolom **NumberOfTrips** karena skewed atau terdapat outlier maka akan diisi oleh median
*   Kolom **NumberOfChildrenVisiting** akan diisi oleh nilai mean
*   Kolom **MonthlyIncome** karena skewed atau terdapat outlier maka diisi oleh nilai median
*   Kolom **TypeofContact** karena bertipe kategori maka akan diisi oleh nilai modus










"""

# Mengisi kolom numerik dengan nilai mean & median
df_mv1['Age'].fillna(df_mv1['Age'].mean(), inplace=True)
df_mv1['DurationOfPitch'].fillna(df_mv1['DurationOfPitch'].median(), inplace=True)
df_mv1['NumberOfFollowups'].fillna(df_mv1['NumberOfFollowups'].mean(), inplace=True)
df_mv1['PreferredPropertyStar'].fillna(df_mv1['PreferredPropertyStar'].mean(), inplace=True)
df_mv1['NumberOfTrips'].fillna(df_mv1['NumberOfTrips'].median(), inplace=True)
df_mv1['NumberOfChildrenVisiting'].fillna(df_mv1['NumberOfChildrenVisiting'].mean(), inplace=True)
df_mv1['MonthlyIncome'].fillna(df_mv1['MonthlyIncome'].median(), inplace=True)

# Mengisi kolom kategorik dengan nilai modus
df_mv1['TypeofContact'].fillna(df_mv1['TypeofContact'].mode()[0], inplace=True)

df_mv1.isnull().sum()

"""### Opsi 2: Mean and Zero Imputation"""

df_mv2 = df.copy()

"""untuk kolom Age, PrefferedPropertyStar, dan MonthlyIncome mengisi data kosong dengan rata - rata karena merupakan nilai numerik absolut"""

df_mv2['Age'].fillna(df_mv2['Age'].mean(), inplace=True)
df_mv2['PreferredPropertyStar'].fillna(df_mv2['PreferredPropertyStar'].mean(), inplace=True)
df_mv2['MonthlyIncome'].fillna(df_mv2['MonthlyIncome'].mean(), inplace=True)

"""untuk kolom NumberOfTrips, NumberOfChildrenVisiting, NumberOfFollowups, dan DurationOfPitch mengunakan nilai 0 untuk mengisi data kosong dikarenakan kemungkinan kekosongan diakibatkan customer melakukan itu sebanyak 0 kali"""

df_mv2['NumberOfTrips'].fillna(0, inplace=True)
df_mv2['NumberOfChildrenVisiting'].fillna(0, inplace=True)
df_mv2['NumberOfFollowups'].fillna(0, inplace=True)
df_mv2['DurationOfPitch'].fillna(0, inplace=True)

"""untuk kolom TypeofContact, menggunakan modus untuk mengisi data kosong dikarenakan termasuk data kategorik"""

df_mv2['TypeofContact'].fillna(df['TypeofContact'].mode()[0], inplace=True)

# Cek jumlah missing value dalam setiap kolom setelah pengisian nilai hilang
df_mv2.isnull().sum()

"""### Opsi 3: KNN Imputer"""

df_mv3 = df.copy()
col_to_impute = ['Age','DurationOfPitch','NumberOfFollowups','PreferredPropertyStar','NumberOfTrips','NumberOfChildrenVisiting','MonthlyIncome']

from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors=2)
df_mv3[col_to_impute] = pd.DataFrame(imputer.fit_transform(df_mv3[col_to_impute]),columns = df_mv3[col_to_impute].columns)

df_mv3['TypeofContact'].fillna(df_mv3['TypeofContact'].mode()[0], inplace=True)

df_mv3.isnull().sum()

"""### Opsi 4: MICE

Opsi yang kami ambil untuk handle missing value adalah menggunakan metode MICE karena imputasi dilakukan menggunakan model regresi.
"""

#df_mv4 = df.copy()
col_to_impute = ['Age','DurationOfPitch','NumberOfFollowups','PreferredPropertyStar','NumberOfTrips','NumberOfChildrenVisiting','MonthlyIncome']

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(random_state=100, max_iter=10)
#df_mv4[col_to_impute] = imputer.fit_transform(df_mv4[col_to_impute])
df_copy[col_to_impute] = imputer.fit_transform(df_copy[col_to_impute])

#df_mv4['TypeofContact'].fillna(df_mv4['TypeofContact'].mode()[0], inplace=True)
df_copy['TypeofContact'].fillna(df_copy['TypeofContact'].mode()[0], inplace=True)

#df_mv4.isnull().sum()
df_copy.isnull().sum()

"""Setelah dilakukan proses imputasi untuk missing values menggunakan metode MICE, dataset sudah tidak ada kolom yang kosong.

## Handling Invalid Values

Berdasarkan hasil pengamatan EDA:
*   Terdapat kesalahan pada penulisan kolom Gender dimana "Fe Male" bermakna sama dengan "Female". Maka "Female" akan direplace menjadi "Female"
*   Terdapat penggunaan istilah yang berbeda pada kolom Marital status yaitu "Unmarried" dan "Single" dimana kedua status tersebut bermakna sama. Maka "Unmarried" akan direplace menjadi "Single"
"""

#Terdapat kesalahan penulisan pada kolom Gender dimana 'Fe Male' seharusnya 'Female'
df_copy['Gender'] = df_copy['Gender'].replace({'Fe Male': 'Female'})

#Terdapat penggunaan istilah yang berbeda pada 'Unmarried' dan 'Single' dimana kedua status itu sama
df_copy['MaritalStatus'] = df_copy['MaritalStatus'].replace({'Unmarried': 'Single'})

"""## Handling Duplicated Data"""

#Cek duplicate value untuk customer id
df_copy.duplicated(subset=['CustomerID']).sum()

"""Berdasarkan hasil pemeriksaan di atas, tidak terdapat data customer id yang terduplikasi"""

# remove CustomerID to see duplicate rows
df_copy.drop('CustomerID', axis=1, inplace=True)
print(f'Jumlah baris data sebelum dilakukan penghapusan data duplikat adalah {df_copy.shape[0]}')

# remove duplicate rows
df_copy.drop_duplicates(inplace=True)
print(f'Jumlah baris data setelah dilakukan penghapusan data duplikat adalah {df_copy.shape[0]}')

"""## Handling Outlier

### Opsi 1: IQR #####
"""

df_out1 = df_copy.copy()

# Menampilkan jumlah baris sebelum memfilter outlier
print(f'Jumlah baris sebelum memfilter outlier adalah {df_out1.shape[0]}')

# Memfilter outlier menggunakan IQR
skewed_cols = ['DurationOfPitch', 'NumberOfTrips', 'MonthlyIncome']
for col in skewed_cols:
    Q1 = df_out1[col].quantile(0.25)
    Q3 = df_out1[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_out1 = df_out1[(df_out1[col] >= lower_bound) & (df_out1[col] <= upper_bound)]

# Menampilkan jumlah baris setelah memfilter outlier
print(f'Jumlah baris setelah memfilter outlier adalah {df_out1.shape[0]}')

"""### Opsi 2: Z-Score

Karena keterbatasan data, metode yang kami pakai untuk handling outlier adalah Z-Score.
"""

# view total rows before filtered
print(f'Jumlah baris sebelum memfilter outlier adalah {df_copy.shape[0]}')

# handle outlier using z-score
filtered_entries = np.array([True] * len(df_copy))
skewed_cols = ['DurationOfPitch', 'NumberOfTrips', 'MonthlyIncome']
for col in skewed_cols:
    zscore = abs(stats.zscore(df_copy[col]))
    filtered_entries = (zscore < 3) & filtered_entries    

# view total rows after filtered
df_copy = df_copy[filtered_entries]
print(f'Jumlah baris setelah memfilter outlier adalah {df_copy.shape[0]}')

"""## Feature Engineering

### Feature Extraction

Pada feature extraction kita akan menggabungkan kolom NumberOfPersonVisiting dan NumberOfChildrenVisting menjadi kolom baru yaitu TotalVisiting.
"""

# Merge column NumberOfPersonVisiting & NumberOfChildrenVisiting

df_copy['TotalVisiting'] = df_copy['NumberOfPersonVisiting'] + df_copy['NumberOfChildrenVisiting']

"""### Feature Transformation ####

Berdasarkan insight yang kita dapat dari EDA, ada beberapa fitur yang berkorelasi dengan target tapi belum berdistribusi normal atau skew positif. Maka kita lakukan log Transformasi atau Standardizing agar distribusi nya normal/ mendekati normal.
"""

# Distribusi MonthlyIncome (nilai asli)
sns.kdeplot(df_copy['MonthlyIncome'])

# Distribusi MonthlyIncome(Setelah Log Transformation)
sns.kdeplot(np.log(df_copy['MonthlyIncome']))

# kita transformasi
df_copy['log_MonthlyIncome']= np.log(df_copy['MonthlyIncome'])

# drop kolom monthlyincome (nilai asli)
df_copy= df_copy.drop(columns='MonthlyIncome')

# Distribusi Age (nilai asli) 
sns.kdeplot(df['Age'])

#karena distribusi sudah normal maka dilakukan standardisasi
df['stand_Age']=StandardScaler().fit_transform(df['Age'].values.reshape(len(df), 1))
sns.kdeplot(df['stand_Age'])

# drop kolom age (nilai asli)
df_copy= df_copy.drop(columns='Age')

# Distribusi DurationOfPitch (nilai asli)
sns.kdeplot(df['DurationOfPitch'])

#Distribusi DurationOfPitch (Setelah Log Transformation)
df['stand_DurationOfPitch']=np.log(df['DurationOfPitch'])
sns.kdeplot(df['stand_DurationOfPitch'])

"""### Feature Encoding

Pada Feature Encoding categorical kita menggunakan librari LabelEncoder bawaan dari sklearn
"""

cat_cols = df_copy.select_dtypes(include='object').columns.tolist()

for col in cat_cols:
    print(f'''Value count kolom {col}:''')
    print(df_copy[col].value_counts())
    print()

# Encoding categorical using library LabelEncoder
from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
df_copy.Occupation=encoder.fit_transform(df_copy.Occupation)
df_copy.Designation=encoder.fit_transform(df_copy.Designation)
df_copy.ProductPitched=encoder.fit_transform(df_copy.ProductPitched)
df_copy.sample(10, random_state=50)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Creating instance of OneHotEncoder
encoder = OneHotEncoder()

# Define the columns to be one-hot encoded
columns_to_encode = ['Gender', 'TypeofContact', 'MaritalStatus']

# Create the ColumnTransformer
ct = ColumnTransformer([('encoder', encoder, columns_to_encode)], remainder='passthrough')

# Apply one-hot encoding to the selected columns
encoded_data = ct.fit_transform(df_copy)

# Create a new DataFrame with the encoded data
encoded_df = pd.DataFrame(encoded_data)

# Update column names for one-hot encoded columns
encoded_columns = ct.named_transformers_['encoder'].get_feature_names_out(columns_to_encode)
new_columns = list(encoded_columns) + list(df_copy.columns.drop(columns_to_encode))
encoded_df.columns = new_columns

# Update df_copy with the encoded DataFrame
df_copy = encoded_df

# Display the resulting dataset
df_copy

df_copy.isnull().sum()

# Cek after encoding categorical
df_copy.sample(5)

"""### Feature Selection ###

#### Opsi 1: ANOVA ####
"""

# separate feature and target
X = df_copy.drop(['ProdTaken'], axis=1, inplace=False) 
y = df_copy['ProdTaken'].values

# import library
from sklearn.feature_selection import f_regression, SelectKBest

# Applying SelectKBest class to extract top 10 best features
fs = SelectKBest(score_func=f_regression,k=10)
# Applying feature selection
fit = fs.fit(X,y)

features_score = pd.DataFrame(fit.scores_)
features = pd.DataFrame(X.columns)
feature_score = pd.concat([features,features_score],axis=1)
# Assigning column names
feature_score.columns = ["Input_Features","F_Score"]
print(feature_score.nlargest(15,columns="F_Score"))

"""Hasil F_Score dari metode ANOVA menunjukkan bahwa tidak ada fitur yang terlalu dominan sehingga relatif terdistribusi.

#### Opsi 2 : Random Forest Feature Performance
"""

#Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Random Forest classifier
rf = RandomForestClassifier()
rf.fit(X, y)

# Extract feature importances
importances = rf.feature_importances_
feature_names = X.columns

# Sort feature importances in descending order
indices = np.argsort(importances)[::-1]
sorted_importances = importances[indices]
sorted_feature_names = feature_names[indices]

# Plot feature importances as a histogram
plt.figure(figsize=(8, 6))
plt.bar(sorted_feature_names, sorted_importances)
plt.xticks(rotation=90)
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

"""Hasil Feature Performance dari metode Random Forest menunjukkan bahwa tidak ada fitur yang terlalu dominan sehingga relatif terdistribusi.

Karena dari dua metode tersebut semua fitur relatif terdistribusi maka kami menggunakan semua kolom sebagai fitur.

## Declaring features and target variable
"""

# separate feature and target
X = df_copy.drop(['ProdTaken','DurationOfPitch','NumberOfFollowups','ProductPitched','PitchSatisfactionScore'], axis=1, inplace=False) 
y = df_copy['ProdTaken'].values

"""## Split the data into train & test set with 70:30 ratio"""

#Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)

# check the shape of X_train and X_test

print(f'Jumlah data latih terdiri dari {len(X_train)} baris')
print(f'Jumlah data uji terdiri dari {len(X_test)} baris')

"""## Handling Imbalance

Perlu dilakukan oversampling karena terdapat ketimpangan pada kolom target. Oversampling akan dilakukan menggunakan metode SMOTE.
"""

from imblearn import under_sampling, over_sampling
X_over_SMOTE, y_over_SMOTE = over_sampling.SMOTE().fit_resample(X_train, y_train)
print(X_over_SMOTE.shape)
print(y_over_SMOTE.shape)

print('Original')
print(pd.Series(y_train).value_counts())
print('\n')
print('SMOTE')
print(pd.Series(y_over_SMOTE).value_counts())

X_train = X_over_SMOTE
y_train = y_over_SMOTE

"""Setelah dilakukan oversampling menggunakan SMOTE maka dapat dilihat bahwa jumlah data pada target sudah terdistrbusi secara merata, oleh karena itu siap untuk dilakukan pemodelan.

# Modelling

Pada tahap modelling kita akan membandingkan beberapa metode yang nantinya kita akan pilih mana model yang menghasilkan performa terbaik. Model dengan performa terbaik yang nantinya akan kita pilih dalam tahap selanjutnya.
"""

# import library untuk modelling
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from yellowbrick.classifier import DiscriminationThreshold

"""## Choose Best Classifier"""

# Model assignment
rfc = RandomForestClassifier(random_state=45)
etc = ExtraTreesClassifier(random_state=45) 
gbc = GradientBoostingClassifier(random_state=45)
bgc = BaggingClassifier(random_state=45)
xgb = XGBClassifier(eval_metric='error',random_state=45)
#dtc = DecisionTreeClassifier()
#abc = AdaBoostClassifier()
#knn = KNeighborsClassifier() 
#logreg = LogisticRegression()
#nb = GaussianNB()
#svm = SVC()
#mlp = MLPClassifier()

# Assign model to a list
#models = [dtc, rfc, abc, etc, gbc, bgc, knn, logreg, nb, svm, xgb, mlp]
models = [rfc, etc, gbc, bgc, xgb]

model_name = []

# Get Classifier names for every model
for name in models:
    names = str(type(name)).split('.')[-1][:-2]
    # Append classifier names to model_name list
    model_name.append(names)

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import RFECV
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV, cross_val_score, RandomizedSearchCV

"""## Cross Validation

Pada tahap ini kita lakukan training pada seluruh data menggunakan cross validation. Hal ini dilakukan untuk memprediksi hasil model yang terbaik dan akan digunakan.
"""

# Cross validation for each model 
rfc_score = cross_val_score(models[0], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
etc_score = cross_val_score(models[1], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
gbc_score = cross_val_score(models[2], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
bgc_score = cross_val_score(models[3], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
xgb_score = cross_val_score(models[4], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#dtc_score = cross_val_score(models[5], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#abc_score = cross_val_score(models[6], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#knn_score = cross_val_score(models[7], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#logreg_score = cross_val_score(models[8], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#nb_score = cross_val_score(models[9], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#svm_score = cross_val_score(models[10], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)
#mlp_score = cross_val_score(models[11], X, y, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)

# List of score per model
#cv_result = [
#    dtc_score, rfc_score, abc_score, etc_score, gbc_score, bgc_score, 
#    knn_score, logreg_score, nb_score, svm_score, xgb_score, mlp_score]
cv_result = [rfc_score, etc_score, gbc_score, bgc_score, xgb_score]

# Create dataframe for score every k-fold
df_cv_result = pd.DataFrame(cv_result, index=model_name)
df_cv_result

"""Indeks 0 - 4 di atas adalah banyaknya percobaan yang dilakukan. Dalam hal ini, kami menggunakan k-fold dengan k = 5 yang artinya melakukan 5 kali percobaan dalam menguji dataset. Hal ini dilakukan agar dapat memprediksi hasil performa model yang terbaik untuk digunakan. Dari hasil tersebut terlihat bahwa `XGBClassifier` memiliki performa yang realtif stabil 0.93... di setiap percobaan sehingga kemungkinan untuk menggunakan XGBClassifier semakin besar. Namun, hasil 5 percobaan di atas nantinya akan dihitung berdasarkan rata-rata setiap model sehingga nantinya model yang memiliki performa terbaik yang akan digunakan."""

# Plot cross validation score
sns.set_style('dark')
plt.figure(figsize=(20,10))
plt.title('Plot cross validation score',fontsize=20, fontweight='bold', pad=20)
sns.lineplot(data=df_cv_result.T)
plt.show()

# Calculate average for every k-fold validation
cv_mean = []
i = 0
for mean in cv_result:
    mean = cv_result[i].mean()
    cv_mean.append(mean)
    i += 1

# Calculate standard deviation for every k-fold validation
cv_std = []
i = 0 
for std in cv_result:
    std = cv_result[i].std()
    cv_std.append(std)
    i += 1

# Average and standard deviation score for each model
df_cv = pd.DataFrame({'score_mean':cv_mean, 'score_std':cv_std}, index=model_name).sort_values(['score_mean', 'score_std'], ascending=[False, True])
df_cv

"""Pada eksperimen diatas didapatkan nilai rata-rata akurasi dan standard deviasinya dari setiap model. Berdasarkan hasil tersebut, XGBClassifier memiliki performa terbaik dengan rata-rata akurasi mencapai 0.898040 dengan standard deviasi 0.012334

## Fit and Evaluation

Selain dengan menggunakan Cross Validation, kami juga melakukan uji nilai 
akurasi pada data training dan testing yang bertujuan untuk melihat akurasi saat data dilakukan training dan testing serta melihat perbedaaannya.
"""

# Create a list to assign a model score
train_score = []
test_score = []

# Create dataframe
df_train_test = pd.DataFrame()
for i in models:
    # Fit each model
    model = i.fit(X_train, y_train)
    # accuracy for training set
    train_score.append(model.score(X_train, y_train))
    # accuracy for testing set
    test_score.append(model.score(X_test, y_test))

# Create a dataframe to store accuracy score
df_avg_score = pd.DataFrame({
    'train score':train_score,
    'test score':test_score},
    index=model_name)

# Create a new column for the difference in accuracy score 
df_avg_score['difference'] = abs(df_avg_score['train score'] - df_avg_score['test score'])
# Sort accuracy by smallest difference
df_avg_score = df_avg_score.sort_values(['train score', 'test score','difference'], ascending=[False, False, True])
df_avg_score

"""Terlihat bahwa `XGBClassifier` memiliki perbedaan nilai akurasi yaitu 13% sehingga kemungkinan `XGBClassifier` bisa digunakan untuk mendapatkan performa yang baik.

Selanjutnya akan dilakukan uji nilai precision, recall, f1-score, dan roc-auc-score pada data training dan testing
"""

# Calculate accuracy, precision, recall, f1-score, and roc-auc-score
def eval(model,X_train,X_test,y_train,y_test):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)
    y_pred_proba_train = model.predict_proba(X_train)
    train_score = round((model.score(X_train, y_train) * 100), 2)
    test_score = round((model.score(X_test, y_test) * 100), 2)
    prec_score = round((precision_score(y_test, y_pred)) * 100, 2)
    rec_score = round((recall_score(y_test, y_pred)) * 100, 2)
    f1 = round(f1_score(y_test, y_pred)*100, 2)
    train_roc_auc = round((roc_auc_score(y_train, y_pred_proba_train[:, 1]) * 100), 2)
    test_roc_auc = round((roc_auc_score(y_test, y_pred_proba[:, 1]) * 100), 2)

    return (train_score,test_score,prec_score,rec_score,f1,train_roc_auc,test_roc_auc)

A=[]
for i in models:
    A.append(eval(i,X_train,X_test,y_train,y_test))

model_eval = pd.DataFrame(A,index=model_name)
model_eval = model_eval.rename(columns={0: 'Training Accuracy', 1: 'Test Accuracy', 2:'Precision Score', 3:'Recall Score', 4:'F1 Score', 5:'roc_auc (train-proba)', 6:'roc_auc (test-proba)'})
model_eval = model_eval.sort_values(['Precision Score'], ascending=[False])
model_eval = model_eval.reset_index()
model_eval = model_eval.rename(columns={'index': 'Model'})
model_eval

"""Pada kasus ini selain nilai akurasi yang kita perhitungkan, nilai presisi juga akan kita perhitungkan karena kita lebih ingin model kita memiliki ketepatan memprediksi potential customer sebesar-besarnya. Total Positive (TP) pada kasus ini merupakan total customer yang diprediksi akan mengambil paket dan benar mengambil paket, sedangkan False Positive (FP) pada kasus ini yaitu customer yang diprediksi akan mengambil paket, tetapi sebenarnya tidak mengambil paket. Karena jumlah potential customer tersebut merupakan total customer yang diprediksi akan membeli yaitu TP+FP dan customer yang diprediksi membeli dan benar membeli adalah TP, maka dipilih nilai TP/(TP+FP) atau nilai presisi sebagai indikator evaluasi terbaik untuk model kami. Dapat dilihat bahwa model Random Forest memiliki nilai presisi tertinggi. Akan tetapi, dikarenakan nilai akurasi mencapai 100% dan terindikasi overfitting, maka model XG Boost kami pilih sebagai model yang memiliki hasil evaluasi paling baik.

## Cross Validation for Some Metrics

Mengukur performa model dengan beberapa metrik, yaitu accuracy, precision, recall, dan f1-score.
"""

skfold = StratifiedKFold(n_splits=5)

# Cross validation for each model
rfc_score = cross_validate(models[0], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
etc_score = cross_validate(models[1], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
gbc_score = cross_validate(models[2], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
bgc_score = cross_validate(models[3], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
xgb_score = cross_validate(models[4], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#dtc_score = cross_validate(models[5], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#abc_score = cross_validate(models[6], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#knn_score = cross_validate(models[7], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#logreg_score = cross_validate(models[8], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#nb_score = cross_validate(models[9], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#svm_score = cross_validate(models[10], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
#mlp_score = cross_validate(models[11], X_train, y_train, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)

#cv_result = [
#    dtc_score, rfc_score, abc_score, etc_score, gbc_score, bgc_score, 
#    knn_score, logreg_score, nb_score, svm_score, xgb_score, mlp_score]
cv_result = [rfc_score, etc_score, gbc_score, bgc_score, xgb_score]

# Average score for each metrics

df_cv_result = pd.DataFrame(cv_result, index=model_name).applymap(np.mean)
df_cv_result = df_cv_result.sort_values(['test_accuracy', 'test_precision'], ascending=False)
df_cv_result = df_cv_result.reset_index()
df_cv_result.rename(columns={'index':'Model'}, inplace=True)
df_cv_result

"""Dari hasil diatas, `XGBClassifier` memiliki nilai akurasi dan dan recall yang tinggi dari keseluruhan model. Pada kasus ini selain nilai akurasi yang kita perhitungkan, recall juga akan kita perhitungkan karena kita lebih ingin model kita dapat mengklasifikasi lebih banyak False Positive (FP) daripada False Negative (FN). FP pada kasus ini yaitu model memprediksi customer akan mengambil paket, tetapi sebenarnya tidak mengambil paket. Maka FP lebih baik daripada FN. FN yaitu model memprediksi customer tidak mengambil paket tetapi sebenarnya mengambil paket, dan hal ini dapat menyebabkan semakin banyak customer yang tidak bisa kita lakukan penawaran lebih lanjut sehingga mereka tidak akan mengambil paket.

Dari keseluruhan proses diatas, maka kami memilih menggunakan model `XGBClassifier` karena memiliki nilai akurasi dan recall yang tertinggi.

## Using XGB Classifier

Pada tahap ini kita akan melakukan perbandingan performa model sebelum dan sesudah dilakukan hyperparameter tuning.

### Default Parameter
"""

# Fit classifier
xgb.fit(X_train, y_train)

# predict test set
y_pred = xgb.predict(X_test)
y_pred_proba = xgb.predict_proba(X_test)
y_pred_proba_train = xgb.predict_proba(X_train)

# Calculate accuracy, precision, recall, and f1-score
train_score = round((xgb.score(X_train, y_train) * 100), 2)
test_score = round((xgb.score(X_test, y_test) * 100), 2)
prec_score = round((precision_score(y_test, y_pred)) * 100, 2)
rec_score = round((recall_score(y_test, y_pred)) * 100, 2)
f1 = round(f1_score(y_test, y_pred)*100, 2)
train_roc_auc = round((roc_auc_score(y_train, y_pred_proba_train[:, 1]) * 100), 2)
test_roc_auc = round((roc_auc_score(y_test, y_pred_proba[:, 1]) * 100), 2)

print('Training Accuracy : {}%'.format(train_score))
print('Test Accuracy : {}%'.format(test_score))
print('Precision Score : {}%'.format(prec_score))
print('Recall Score : {}%'.format(rec_score))
print('F1 Score : {}%'.format(f1))
print('roc_auc (train-proba) : {}%'.format(train_roc_auc))
print('roc_auc (test-proba) : {}%'.format(test_roc_auc))

pd.DataFrame({
    'train_acc':[train_score],
    'test_acc':[test_score],
    'precision':[prec_score],
    'recall':[rec_score],
    'f1-score':[f1],
    'roc_auc (train-proba)':[train_roc_auc],
    'roc_auc (test-proba)':[test_roc_auc]}, index=['Default XGB'])

"""### Hyperparameter Tuning

Hyperparameter Tuning dilakukan untuk memilih parameter terbaik yang akan digunakan. Metode tuning yang digunakan yaitu Random Seach CV. Random Search CV dipilih karena lebih mudah dalam penggunaannya.
"""

params = {
    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],
    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],
    'tree_method' : ['auto', 'exact', 'approx', 'hist'],

    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],
    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],

    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],
    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]
    #'n_estimators': [10, 20, 50, 100, 200, 500, 1000, 1200, 1500, 1800, 1900, 2000, 2100, 3000],
    #'criterion': ['gini', 'entropy'],
    #'max_depth': [1, 2, 5, 8, 13, 21, 34, 53, 54, 55, 89, None]
}

xgb_tuned = RandomizedSearchCV(
    estimator=XGBClassifier(), 
    param_distributions=params, 
    n_iter=10,
    scoring='recall', 
    cv=5, 
    n_jobs=-1)

xgb_tuned.fit(X_train, y_train)

# best estimator for xgboost
xgb_tuned.best_estimator_

# Predict test set
y_pred_tuned = xgb_tuned.predict(X_test)
y_pred_proba_tuned = xgb.predict_proba(X_test)
y_pred_proba_train_tuned = xgb.predict_proba(X_train)

#Calculate accuracy, precision, recall, and f1-score
train_score_tuned = round((xgb_tuned.score(X_train, y_train) * 100), 2)
test_score_tuned = round((xgb_tuned.score(X_test, y_test) * 100), 2)
prec_score_tuned = round((precision_score(y_test, y_pred_tuned)) * 100, 2)
rec_score_tuned = round((recall_score(y_test, y_pred_tuned)) * 100, 2)
f1_tuned = round(f1_score(y_test, y_pred_tuned)*100, 2)
train_roc_auc_tuned = round((roc_auc_score(y_train, y_pred_proba_train_tuned[:, 1]) * 100), 2)
test_roc_auc_tuned = round((roc_auc_score(y_test, y_pred_proba_tuned[:, 1]) * 100), 2)

print('Training Accuracy : {}%'.format(train_score_tuned))
print('Test Accuracy : {}%'.format(test_score_tuned))
print('Precision Score : {}%'.format(prec_score_tuned))
print('Recall Score : {}%'.format(rec_score_tuned))
print('F1 Score : {}%'.format(f1_tuned))
print('roc_auc (train-proba) : {}%'.format(train_roc_auc_tuned))
print('roc_auc (test-proba) : {}%'.format(test_roc_auc_tuned))

pd.DataFrame({
    'train_acc':[train_score, train_score_tuned],
    'test_acc':[test_score, test_score_tuned],
    'precision':[prec_score, prec_score_tuned],
    'recall':[rec_score, rec_score_tuned],
    'f1-score':[f1, f1_tuned],
    'roc_auc (train-proba)':[train_roc_auc, train_roc_auc_tuned],
    'roc_auc (test-proba)':[test_roc_auc, test_roc_auc_tuned]}, index=['xgb', 'xgb_tuned'])

"""Setelah dilakukan hyperparameter tuning, ternyata model mengalami penurunan nilai akurasi pada data test dan semakin memperbesar perbedaan nilai akurasi data training dan data test sehingga terindikasi overfitting. Karena secara keseluruhan model mengalami penurunan performa, maka kita akan menggunakan default parameter.

## Model Evaluation

Lakukan evaluasi terhadap model, seperti melihat confusion matrix, classification report dan feature importance hasil training.

### Confusion Matrix
"""

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='YlGnBu')
plt.title('Tabel Confusion Matrix')
plt.show()

# Classification report model
cr = classification_report(y_test, y_pred)
print(cr)

# Precision Recall Curve
y_pred = xgb.predict_proba(X_test)[:,1]
prec, recall, _ = precision_recall_curve(y_test, y_pred, pos_label=xgb.classes_[1])
pr_display = PrecisionRecallDisplay(prec, recall)

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=xgb.classes_[1])
roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)

# roc auc score
auc_score = roc_auc_score(y_test, y_pred)
print(f'ROC AUC Score is : {auc_score}')

# Plotting Precision Recall and ROC Curve
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

pr_display.plot(ax=ax1)
roc_display.plot(ax=ax2)
ax1.set_title('Precision Recall Curve', fontsize=15, fontweight='bold')
ax2.set_title('ROC Curve', fontsize=15, fontweight='bold')
plt.show()

"""## Feature Importance"""

feature_importance = pd.DataFrame({
    'Features':X.columns,
    'Importance':xgb.feature_importances_}).sort_values('Importance', ascending=False)

plt.figure(figsize=(15, 12))
sns.barplot(data=feature_importance, x='Importance', y='Features', color='forestgreen')

# function to add value labels
def addlabels(x,y):
    for i in range(len(x)):
        plt.text(x[i],i, round(x[i], 4), fontsize=13, fontweight='bold')

x = list(feature_importance['Importance'])
y = list(feature_importance['Features'])

plt.title('Feature Importance', fontsize=20, color='black', pad=15, fontweight='bold')
plt.yticks(fontsize=15)
addlabels(x, y)
plt.show()

"""Beberapa Feature Importance hasil dari modelling XGBoost diantaranya adalah **Passport, MaritalStatus_Single, OwnCar, MaritalStatus_Divorce, Designation, NumberOfChildrenVisiting, Occupation, CityTier, PreferredPropertyStar, MaritalStatus_Married, TypeOfContact_Company Invited, Gender_Female, NumberOfTrips, NumberOfPersonVisiting, TotalVisiting dan log_MonthlyIncome**"""

!pip install shap

from xgboost import XGBClassifier
dt = XGBClassifier()
dt.fit(X_train, y_train)

import shap
explainer = shap.TreeExplainer(dt)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test) # Summary shap value terhadap label positive

"""Dari observasi SHAP diatas dapat kita ambil kesimpulan bahwa pelanggan yang memiliki pasport, tinggal di citytier 3 dan belum menikah atau berstatus single berkorelasi positif terhadap ProdTaken(Target) yang mana jika difollow-up lebih lanjut kemungkinan besar akan membeli produk. """

y_pred_final = pd.DataFrame({'ProdTaken':y_test, 'ProdTaken_Prob':y_pred})

"""# Results

## ProdTaken Before Modeling
"""

#check proportion of ProdTaken
import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
plt.pie(
        df["ProdTaken"].value_counts(),
        autopct='%.2f',
        explode=[0.1,0],
        labels=["No","Yes"], 
        shadow=True, 
        textprops={'fontsize': 14},
        colors=["gray","red"], 
        startangle=35)

plt.title("ProdTaken sebelum modelling",fontsize=20, fontweight='bold', pad=20)
plt.legend(fontsize=12, loc="best")
plt.show()

"""## ProdTaken After Modeling"""

#check proportion of ProdTaken
import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
plt.pie(
        y_pred_final["ProdTaken"].value_counts(),
        autopct='%.2f',
        explode=[0.1,0],
        labels=["No","Yes"], 
        shadow=True, 
        textprops={'fontsize': 14},
        colors=["gray","red"], 
        startangle=35)

plt.title("ProdTaken setelah modelling",fontsize=20, fontweight='bold', pad=20)
plt.legend(fontsize=12, loc="best")
plt.show()